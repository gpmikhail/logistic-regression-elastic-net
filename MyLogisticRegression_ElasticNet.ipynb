{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MyLogisticRegression_ElasticNet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Xa2hZySlKx11"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def logit(x, w):\n",
        "    return np.dot(x, w)\n",
        "\n",
        "def sigmoid(h):\n",
        "    return 1. / (1 + np.exp(-h))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def generate_batches(X, y, batch_size):\n",
        "    assert len(X) == len(y)\n",
        "    np.random.seed(42)\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "    perm = np.random.permutation(len(X))\n",
        "    ind = []\n",
        "    start = 0\n",
        "    end = batch_size\n",
        "\n",
        "    for batch_start in range(len(X)//batch_size):\n",
        "      ind = perm[start:end]\n",
        "      yield (X[np.array(ind)], y[np.array(ind)])\n",
        "      ind = []\n",
        "      start += batch_size\n",
        "      end += batch_size"
      ],
      "metadata": {
        "id": "SzZtlizLMWPc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyLogisticRegression(object):\n",
        "    def __init__(self):\n",
        "        self.w = None\n",
        "    \n",
        "    def fit(self, X, y, epochs=10, lr=0.1, batch_size=100):\n",
        "        n, k = X.shape        \n",
        "        if self.w is None:\n",
        "            np.random.seed(42)\n",
        "            \n",
        "            self.w = np.random.randn(k + 1)\n",
        "\n",
        "        X_train = np.concatenate((np.ones((n, 1)), X), axis=1)\n",
        "        \n",
        "        losses = []\n",
        "        \n",
        "       \n",
        "\n",
        "        for i in range(epochs):\n",
        "            for X_batch, y_batch in generate_batches(X_train, y, batch_size):\n",
        "                \n",
        "                predictions = self._predict_proba_internal(X_batch) \n",
        "                loss = self.__loss(y_batch, predictions) \n",
        "\n",
        "                assert (np.array(loss).shape == tuple()), \"Лосс должен быть скаляром!\" \n",
        "\n",
        "                losses.append(loss)\n",
        "                \n",
        "                \n",
        "                self.w -= lr * self.get_grad(X_batch, y_batch, predictions)\n",
        "\n",
        "        return losses\n",
        "    \n",
        "    def get_grad(self, X_batch, y_batch, predictions):\n",
        "      \n",
        "        \n",
        "        grad_basic = X_batch.transpose() @ (predictions-y_batch)\n",
        "        assert grad_basic.shape == (X_batch.shape[1],) , \"Градиенты должны быть столбцом из k_features + 1 элементов\"\n",
        "\n",
        "        return grad_basic\n",
        "        \n",
        "    def predict_proba(self, X):\n",
        "        n, k = X.shape\n",
        "        X_ = np.concatenate((np.ones((n, 1)), X), axis=1)\n",
        "        return sigmoid(logit(X_, self.w))\n",
        "\n",
        "    def _predict_proba_internal(self, X): \n",
        "\n",
        "        return sigmoid(logit(X, self.w))\n",
        "\n",
        "    def predict(self, X, threshold=0.5):\n",
        "        return self.predict_proba(X) >= threshold\n",
        "    \n",
        "    def get_weights(self):\n",
        "        return self.w.copy() \n",
        "      \n",
        "    def __loss(self, y, p):  \n",
        "        p = np.clip(p, 1e-10, 1 - 1e-10)\n",
        "        return -np.sum(y * np.log(p) + (1 - y) * np.log(1 - p))"
      ],
      "metadata": {
        "id": "KiP49_F-Kyyw"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tests**"
      ],
      "metadata": {
        "id": "SCymD1Z2L1CJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m = MyLogisticRegression()\n",
        "X = np.array([[1, 3, 4], [1, -5, 6], [-3, 5, 3]])\n",
        "X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
        "y = np.array([1, 0, 1])\n",
        "preds = np.array([.55, .22, .85])\n",
        "grads = m.get_grad(X, y, preds) \n",
        "assert np.allclose(grads, np.array([-0.38,  0.22, -3.2 , -0.93])), \"Что-то не так!\""
      ],
      "metadata": {
        "id": "QBu8Zj0nL4q4"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "m = MyLogisticRegression()\n",
        "X = np.random.rand(100,3)\n",
        "y = np.random.randint(0, 1, size=(100,))\n",
        "preds = np.random.rand(100)\n",
        "grads = m.get_grad(X, y, preds) \n",
        "assert np.allclose(grads, np.array([23.8698149, 25.27049356, 24.4139452])), \"Что-то не так!\""
      ],
      "metadata": {
        "id": "2hsSbSErL5pV"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ElasticNet Logistic Regression**"
      ],
      "metadata": {
        "id": "mr8M930gL-6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyElasticLogisticRegression(MyLogisticRegression):\n",
        "    def __init__(self, l1_coef, l2_coef):\n",
        "        self.l1_coef = l1_coef\n",
        "        self.l2_coef = l2_coef\n",
        "        self.w = None\n",
        "    \n",
        "    def get_grad(self, X_batch, y_batch, predictions):\n",
        "  \n",
        "\n",
        "        grad_basic = X_batch.transpose() @ (predictions-y_batch) \n",
        "\n",
        "        grad_l1 = self.l1_coef * np.sign(self.w) \n",
        "        grad_l2 = 2 * self.l2_coef * self.w   \n",
        "        grad_l1[0] = 0\n",
        "        grad_l2[0] = 0\n",
        "\n",
        "        \n",
        "        assert grad_l1[0] == grad_l2[0] == 0, \"Bias в регуляризационные слагаемые не входит!\"\n",
        "        assert grad_basic.shape == grad_l1.shape == grad_l2.shape == (X_batch.shape[1],) , \"Градиенты должны быть столбцом из k_features + 1 элементов\"\n",
        "        \n",
        "        return grad_basic + grad_l1 + grad_l2"
      ],
      "metadata": {
        "id": "mWmV9hv4MDpX"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tests**"
      ],
      "metadata": {
        "id": "xQe3B4XGMLOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m = MyElasticLogisticRegression(.2,.2) \n",
        "X = np.array([[1, 3, 4], [1, -5, 6], [-3, 5, 3]])\n",
        "X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
        "y = np.array([1, 0, 1])\n",
        "preds = np.array([.55, .22, .85])\n",
        "m.w = np.array([1,1,1,1])\n",
        "grads = m.get_grad(X, y, preds)\n",
        "assert np.allclose(grads, np.array([-0.38,  0.82, -2.6 , -0.33])), \"Что-то не так!\""
      ],
      "metadata": {
        "id": "pLS7K-4DMNTr"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "m = MyElasticLogisticRegression(.2, .2)\n",
        "X = np.random.rand(100,3)\n",
        "X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
        "y = np.random.randint(0, 1, size=(100,))\n",
        "preds = np.random.rand(100)\n",
        "m.w = np.array([1,1,1,1])\n",
        "grads = m.get_grad(X, y, preds)\n",
        "assert np.allclose(grads, np.array([49.11489408, 24.4698149, 25.87049356, 25.0139452])), \"Что-то не так!\""
      ],
      "metadata": {
        "id": "KRI2ucoKMQTz"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}